{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import textacy.preprocessing as tprep\n",
    "from transformers import DistilBertTokenizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "data_path = os.path.join(data_dir, \"raw\", \"anime_data.csv\")\n",
    "raw_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For autoencoder training we drop all NaNs\n",
    "raw_data = raw_data.dropna()\n",
    "# Drop irrelevant columns\n",
    "raw_data = raw_data.drop(columns=['created_at', 'updated_at', 'related_manga', 'recommendations', 'main_picture.medium', 'main_picture.large'])\n",
    "train_data, test_data = train_test_split(raw_data, test_size=0.2, random_state=42)\n",
    "train = False\n",
    "data = train_data if train else test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'id,' # Anime ID (integer)\\\n",
    "ðŸ›‘'title,' # Anime title (string)\\\n",
    "âœ…'synopsis,' # Anime synopsis (string or null)\\\n",
    "âœ…'mean,' # Mean score (float or null)\\\n",
    "âœ…'popularity,' # Popularity rank (integer or null)\\\n",
    "ðŸ›‘'num_list_users,' # Number of users who have the anime in their list (integer)\\\n",
    "âœ…'num_scoring_users,' # Number of users who have scored the anime (integer)\\\n",
    "âœ…'nsfw,' # NSFW classification (white=sfw, gray=partially, black=nsfw) (string or null)\\\n",
    "âœ…'genres,' # Genres (array of objects)\\\n",
    "âœ…'studios,' # Studios (array of objects)\\\n",
    "âœ…'num_episodes,' # Number of episodes (integer)\\\n",
    "âœ…'average_episode_duration,' # Average duration of an episode (integer or null)\\\n",
    "âœ…'status,' # Airing status (string)\\\n",
    "âœ…'rating,' # Age rating (string or null) (g, pg, pg_13, r, r+, rx)\\\n",
    "âœ…'source,' # Source (string or null)\\\n",
    "âœ…'media_type,' # Media type (string)\\\n",
    "ðŸ›‘'created_at,' # Date of creation (string <date-time>)\\\n",
    "ðŸ›‘'updated_at,' # Date of last update (string <date-time>)\\\n",
    "âœ…'start_season,' # Start season (object or null)\\\n",
    "âœ…'start_date,' # Start date (string or null)\\\n",
    "âœ…'end_date,' # End date (string or null)\\\n",
    "ðŸ›‘'related_anime,' # Related anime (array of objects)\\\n",
    "ðŸ›‘'related_manga,' # Related manga (array of objects)\\\n",
    "ðŸ›‘'recommendations,' # Recommendations (array of objects)\\\n",
    "âœ…'statistics' # Statistics (object or null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Date/Season Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dates\n",
    "['start_season,' # Start season (object or null)]\\\n",
    "['start_date,' # Start date (string or null)]\\\n",
    "['end_date,' # End date (string or null)]\\\n",
    "['start_season.year,' # Start year (object or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_date_convert(date) -> datetime.date:\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    if type(date) is float:\n",
    "        return datetime.strptime(str(int(date)), '%Y').date()\n",
    "    if type(date) is str:\n",
    "        if re.compile(\"\\d{4}-\\d{2}-\\d{2}\").match(date):\n",
    "            return datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        elif re.compile(\"\\d{4}-\\d{2}\").search(date):\n",
    "            return datetime.strptime(date, '%Y-%m').date()\n",
    "        else:\n",
    "            return datetime.strptime(date, '%Y').date()\n",
    "    raise ValueError(f\"Invalid date format: {date}, {type(date)}\")\n",
    "\n",
    "def time_diff(start_date, end_date):\n",
    "    if pd.isna(start_date) or pd.isna(end_date):\n",
    "        return None\n",
    "    if start_date <= end_date:\n",
    "        return (end_date - start_date).days\n",
    "    else:\n",
    "        return (start_date - end_date).days\n",
    "\n",
    "# Convert dates to datetime objects\n",
    "data['start_date'] = data['start_date'].apply(safe_date_convert)\n",
    "data['end_date'] = data['end_date'].apply(safe_date_convert)\n",
    "# Calculate time difference\n",
    "data['time_diff'] = data.apply(lambda x: time_diff(x['start_date'], x['end_date']), axis=1)\n",
    "data = data.drop(columns=['start_date', 'end_date'])\n",
    "# Scale time_diff\n",
    "td_scaler = PowerTransformer()\n",
    "data['time_diff'] = td_scaler.fit_transform(data['time_diff'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_scaler = MinMaxScaler()\n",
    "data['start_season.year'] = year_scaler.fit_transform(data['start_season.year'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Start Season\n",
    "['start_season,' (object or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_encode(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data\n",
    "\n",
    "season_encoder = LabelEncoder()\n",
    "data['start_season.season'] = season_encoder.fit_transform(data['start_season.season'])\n",
    "\n",
    "# Apply the cyclical_encode function to create sine and cosine features\n",
    "data = cyclical_encode(data, 'start_season.season', max_val=len(season_encoder.classes_))\n",
    "data = data.drop(columns=['start_season.season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Synopsis\n",
    "['synopsis,' (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = unicodedata.normalize('NFKC', text)  # Unicode normalization\n",
    "    text = text.replace('\\u2013', '\\u002d')  # Replace en dash with hyphen\n",
    "    text = text.replace('\\u00d7', '\\u0078')  # Replace multiplication sign with x\n",
    "    text = tprep.normalize.hyphenated_words(text)  # Normalize hyphenated words\n",
    "    text = tprep.normalize.quotation_marks(text)  # Normalize quotation marks\n",
    "    text = tprep.normalize.bullet_points(text)  # Normalize bullet points\n",
    "    text = tprep.normalize.whitespace(text)  # Normalize whitespace\n",
    "    text = tprep.remove.accents(text)  # Remove accents\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags if any\n",
    "    text = re.sub(r\"\\([\\s+]?source.*?\\)+\", \"\", text, flags=re.IGNORECASE)  # Remove source citations\n",
    "    text = re.sub(r\"\\[Writ.*?by.*?\\]\", \"\", text)  # Remove MAL citations\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "    text = text.strip()  # Strip whitespace from the beginning and the end\n",
    "    return text\n",
    "\n",
    "data['synopsis'] = data['synopsis'].apply(clean_text)\n",
    "#tokenizer = DistilBertTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "#data['tokenized_synopsis'] = data['synopsis'].apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=512, return_tensors='pt') if not pd.isna(x) else None)\n",
    "#data = data.drop(columns=['synopsis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Title & Related Anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['related'] = data['title'] + ' ' + data['related_anime'].apply(lambda x: ' '.join([entry['node']['title'] for entry in ast.literal_eval(x)]))\n",
    "data = data.drop(columns=['title', 'related_anime'])\n",
    "#data['related'] = data['related'].apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=512, return_tensors='pt') if not pd.isna(x) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Synopsis and Related Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = data[['id', 'related', 'synopsis']]\n",
    "name = 'train_data' if train else 'test_data'\n",
    "text_data_filename = os.path.join(data_dir, \"processed\", f'text_{name}')\n",
    "with open(text_data_filename + '.pkl', 'wb') as f:\n",
    "    pickle.dump(text_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Tag & Label Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Genre Lables\n",
    "['genres,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = set()\n",
    "\n",
    "def process(entry):\n",
    "    genres_set = set(genre['name'] for genre in entry)\n",
    "    unique_genres.update(genres_set)\n",
    "    return genres_set\n",
    "\n",
    "data['genres'] = data['genres'].apply(ast.literal_eval)\n",
    "data['genres'] = data['genres'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mlb = MultiLabelBinarizer()\n",
    "genre_mlb.fit([unique_genres])\n",
    "\n",
    "data['genres'] = data['genres'].apply(lambda x: np.squeeze(genre_mlb.transform([x])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Studio Labels\n",
    "['studios,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_studios = set()\n",
    "\n",
    "def process(entry):\n",
    "    studios_set = set(studio['name'] for studio in entry)\n",
    "    unique_studios.update(studios_set)\n",
    "    return studios_set\n",
    "\n",
    "data['studios'] = data['studios'].apply(ast.literal_eval)\n",
    "data['studios'] = data['studios'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_mlb = MultiLabelBinarizer()\n",
    "studio_mlb.fit([unique_studios])\n",
    "\n",
    "data['studios'] = data['studios'].apply(lambda x: np.squeeze(studio_mlb.transform([x])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess NSFW Tag\n",
    "['nsfw,' (white=sfw, gray=partially, black=nsfw) (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsfw_encoder = LabelEncoder()\n",
    "nsfw_encoder.fit(data['nsfw'].unique())\n",
    "data['nsfw'] = nsfw_encoder.transform(data['nsfw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Source\n",
    "['source,' (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_encoder = LabelEncoder()\n",
    "source_encoder.fit(data['source'].unique())\n",
    "data['source'] = source_encoder.transform(data['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Status\n",
    "['status,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_encoder = LabelEncoder()\n",
    "status_encoder.fit(data['status'].unique())\n",
    "data['status'] = status_encoder.transform(data['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Media Type\n",
    "['media_type,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might want to change media_type to better reflect the data\n",
    "# We might use the following rules:\n",
    "# movie = 'avg_ep_dur'>1800\n",
    "# tv = ('avg_ep_dur'<=1800 & 'num_episodes'>=6)\n",
    "# special = ('avg_ep_dur'<=1800 & 'num_episodes'<6) | ('avg_ep_dur' < 240)\n",
    "# This covers all cases, but the duration and num_ep thresholds seem suboptimal after some testing\n",
    "# thus we skip this for now\n",
    "def filter_media_type(anime):\n",
    "    d = anime['average_episode_duration']\n",
    "    n = anime['num_episodes'] \n",
    "    if d > 1800:\n",
    "        anime['media_type'] = 'movie'\n",
    "    elif d <= 1800 and n >= 6:\n",
    "        anime['media_type'] = 'tv'\n",
    "    elif (d <= 1800 and n < 6) or d < 240:\n",
    "        anime['media_type'] = 'special'\n",
    "    return anime\n",
    "\n",
    "data['media_type'] = data['media_type'].apply(lambda x: 'special' if x in {'ona', 'ova', 'tv_special'} else x)\n",
    "#data.loc[(data['time_diff'] > 0) & (data['media_type'] == 'movie'), 'media_type'] = 'special'\n",
    "#data = data.apply(lambda x: filter_media_type(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_type_encoder = LabelEncoder()\n",
    "media_type_encoder.fit(data['media_type'].unique())\n",
    "data['media_type'] = media_type_encoder.transform(data['media_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Rating\n",
    "['rating,' (string or null) (g, pg, pg_13, r, r+, rx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_map = {\n",
    "    \"g\": 0,\n",
    "    \"pg\": 1,\n",
    "    \"pg_13\": 2,\n",
    "    \"r\": 3,\n",
    "    \"r+\": 4,\n",
    "    \"rx\": 5\n",
    "}\n",
    "data['rating'] = data['rating'].map(rating_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Numerical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Mean\n",
    "['mean,' (float or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature looks similar to a normal distribution, so we try a standard scaler\n",
    "mean_scaler = StandardScaler()\n",
    "data['mean'] = mean_scaler.fit_transform(data['mean'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Popularity\n",
    "['popularity,' (integer or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of this feature seems to get messed up for anything other then standard scaler\n",
    "popularity_scaler = StandardScaler()\n",
    "data['popularity'] = popularity_scaler.fit_transform(data['popularity'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Number of Users Who Have Scored The Anime\n",
    "['num_scoring_users,' (integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature exhibits a long tail distribution, we try power transformer (yeo-johnson)\n",
    "# This might not the best way to handle this feature\n",
    "# Perhaps try https://arxiv.org/abs/2111.05956#:~:text=The%20visual%20world%20naturally%20exhibits,models%20based%20on%20deep%20learning.\n",
    "\n",
    "popularity_scaler = PowerTransformer()\n",
    "data['num_scoring_users'] = popularity_scaler.fit_transform(data['num_scoring_users'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Number of Episodes\n",
    "['num_episodes,' (integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature exhibits a long tail distribution, we again try power transformer (yeo-johnson)\n",
    "num_episodes_scaler = PowerTransformer()\n",
    "data['num_episodes'] = num_episodes_scaler.fit_transform(data['num_episodes'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Average Episode Duration\n",
    "['average_episode_duration,' (integer or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature might also benefit from power transformer (yeo-johnson)\n",
    "avg_ep_scaler = PowerTransformer()\n",
    "data['average_episode_duration'] = avg_ep_scaler.fit_transform(data['average_episode_duration'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Statistics\n",
    "'statistics' (object or null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature 'num_list_users' contains inconsistent data\n",
    "# We will drop this feature, and instead create a new feature 'statistics.sum'\n",
    "data = data.drop(columns=['num_list_users'])\n",
    "#data['statistics.sum'] = data['statistics.status.watching'] + data['statistics.status.completed'] + data['statistics.status.on_hold'] + data['statistics.status.dropped'] + data['statistics.status.plan_to_watch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "watching_scaler = PowerTransformer()\n",
    "data['statistics.status.watching'] = watching_scaler.fit_transform(data['statistics.status.watching'].values.reshape(-1, 1))\n",
    "completed_scaler = PowerTransformer()\n",
    "data['statistics.status.completed'] = completed_scaler.fit_transform(data['statistics.status.completed'].values.reshape(-1, 1))\n",
    "on_hold_scaler = PowerTransformer()\n",
    "data['statistics.status.on_hold'] = on_hold_scaler.fit_transform(data['statistics.status.on_hold'].values.reshape(-1, 1))\n",
    "dropped_scaler = PowerTransformer()\n",
    "data['statistics.status.dropped'] = dropped_scaler.fit_transform(data['statistics.status.dropped'].values.reshape(-1, 1))\n",
    "plan_to_watch_scaler = PowerTransformer()\n",
    "data['statistics.status.plan_to_watch'] = plan_to_watch_scaler.fit_transform(data['statistics.status.plan_to_watch'].values.reshape(-1, 1))\n",
    "num_list_users_scaler = PowerTransformer()\n",
    "data['statistics.num_list_users'] = num_list_users_scaler.fit_transform(data['statistics.num_list_users'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Processed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'train_data' if train else 'test_data'\n",
    "file_path = os.path.join(data_dir, \"processed\", name)\n",
    "data.to_csv(file_path + '.csv', index=False)\n",
    "with open(file_path + '.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

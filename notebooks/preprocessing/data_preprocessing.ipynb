{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "data_path = os.path.join(data_dir, \"raw\", \"anime_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'id,' # Anime ID (integer)\\\n",
    "'title,' # Anime title (string)\\\n",
    "✅'synopsis,' # Anime synopsis (string or null)\\\n",
    "'mean,' # Mean score (float or null)\\\n",
    "'popularity,' # Popularity rank (integer or null)\\\n",
    "'num_list_users,' # Number of users who have the anime in their list (integer)\\\n",
    "'num_scoring_users,' # Number of users who have scored the anime (integer)\\\n",
    "✅'nsfw,' # NSFW classification (white=sfw, gray=partially, black=nsfw) (string or null)\\\n",
    "✅'genres,' # Genres (array of objects)\\\n",
    "✅'studios,' # Studios (array of objects)\\\n",
    "'num_episodes,' # Number of episodes (integer)\\\n",
    "'average_episode_duration,' # Average duration of an episode (integer or null)\\\n",
    "✅'status,' # Airing status (string)\\\n",
    "✅'rating,' # Age rating (string or null) (g, pg, pg_13, r, r+, rx)\\\n",
    "✅'source,' # Source (string or null)\\\n",
    "✅'media_type,' # Media type (string)\\\n",
    "'created_at,' # Date of creation (string <date-time>)\\\n",
    "'updated_at,' # Date of last update (string <date-time>)\\\n",
    "'start_season,' # Start season (object or null)\\\n",
    "'start_date,' # Start date (string or null)\\\n",
    "'end_date,' # End date (string or null)\\\n",
    "'related_anime,' # Related anime (array of objects)\\\n",
    "'related_manga,' # Related manga (array of objects)\\\n",
    "'recommendations,' # Recommendations (array of objects)\\\n",
    "'statistics') # Statistics (object or null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'synopsis', 'mean', 'popularity', 'num_list_users',\n",
       "       'num_scoring_users', 'nsfw', 'genres', 'studios', 'num_episodes',\n",
       "       'average_episode_duration', 'status', 'rating', 'source', 'media_type',\n",
       "       'created_at', 'updated_at', 'start_date', 'end_date', 'related_anime',\n",
       "       'related_manga', 'recommendations', 'main_picture.medium',\n",
       "       'main_picture.large', 'start_season.year', 'start_season.season',\n",
       "       'statistics.status.watching', 'statistics.status.completed',\n",
       "       'statistics.status.on_hold', 'statistics.status.dropped',\n",
       "       'statistics.status.plan_to_watch', 'statistics.num_list_users'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Start Season ['start_season,' (object or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['start_season.season'] = raw_data['start_season.season'].fillna('Unknown')\n",
    "raw_data['start_season.year'] = raw_data['start_season.year'].fillna(raw_data['start_season.year'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_encoder = LabelEncoder()\n",
    "season_encoder.fit(raw_data['start_season.season'].unique())\n",
    "raw_data['start_season.season'] = season_encoder.transform(raw_data['start_season.season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Synopsis Data ['synopsis,' (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = unicodedata.normalize('NFKC', text)  # Unicode normalization\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags if any\n",
    "    text = re.sub(r\"\\(.*source.*\\)\", \"\", text, flags=re.IGNORECASE)  # Remove source citations\n",
    "    text = re.sub(r\"\\[.*MAL.*\\]\", \"\", text)  # Remove MAL citations\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "    text = text.strip()  # Strip whitespace from the beginning and the end\n",
    "    return text\n",
    "\n",
    "raw_data['synopsis'] = raw_data['synopsis'].fillna(\"\")\n",
    "raw_data['synopsis'] = raw_data['synopsis'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Genre Lables ['genres,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['genres'] = raw_data['genres'].fillna('[]')\n",
    "raw_data['genres'] = raw_data['genres'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = set()\n",
    "\n",
    "def process(entry):\n",
    "    genres_set = set(genre['name'] for genre in entry)\n",
    "    unique_genres.update(genres_set)\n",
    "    return genres_set\n",
    "\n",
    "raw_data['genres'] = raw_data['genres'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mlb = MultiLabelBinarizer()\n",
    "genre_mlb.fit([unique_genres])\n",
    "\n",
    "raw_data['genres'] = raw_data['genres'].apply(lambda x: np.squeeze(genre_mlb.transform([x])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Studio Labels ['studios,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['studios'] = raw_data['studios'].fillna('[]')\n",
    "raw_data['studios'] = raw_data['studios'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_studios = set()\n",
    "\n",
    "def process(entry):\n",
    "    studios_set = set(studio['name'] for studio in entry)\n",
    "    unique_studios.update(studios_set)\n",
    "    return studios_set\n",
    "\n",
    "raw_data['studios'] = raw_data['studios'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_mlb = MultiLabelBinarizer()\n",
    "studio_mlb.fit([unique_studios])\n",
    "\n",
    "raw_data['studios'] = raw_data['studios'].apply(lambda x: np.squeeze(studio_mlb.transform([x])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess NSFW Tag ['nsfw,' (white=sfw, gray=partially, black=nsfw) (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['nsfw'] = raw_data['nsfw'].fillna(\"Unknown\")\n",
    "nsfw_encoder = LabelEncoder()\n",
    "nsfw_encoder.fit(raw_data['nsfw'].unique())\n",
    "raw_data['nsfw'] = nsfw_encoder.transform(raw_data['nsfw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Source ['source,' (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['source'] = raw_data['source'].fillna(\"Unknown\")\n",
    "source_encoder = LabelEncoder()\n",
    "source_encoder.fit(raw_data['source'].unique())\n",
    "raw_data['source'] = source_encoder.transform(raw_data['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Status ['status,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_encoder = LabelEncoder()\n",
    "status_encoder.fit(raw_data['status'].unique())\n",
    "raw_data['status'] = status_encoder.transform(raw_data['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Media Type ['media_type,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_type_encoder = LabelEncoder()\n",
    "media_type_encoder.fit(raw_data['media_type'].unique())\n",
    "raw_data['media_type'] = media_type_encoder.transform(raw_data['media_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Rating ['rating,' (string or null) (g, pg, pg_13, r, r+, rx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_map = {\n",
    "    \"g\": 0,\n",
    "    \"pg\": 1,\n",
    "    \"pg_13\": 2,\n",
    "    \"r\": 3,\n",
    "    \"r+\": 4,\n",
    "    \"rx\": 5\n",
    "}\n",
    "raw_data['rating'] = raw_data['rating'].fillna(\"Unknown\")\n",
    "raw_data['rating'] = raw_data['rating'].map(rating_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Numerical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Num Episodes ['num_episodes,' (integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  28,   64,   24,   51,   10,    1,  148,   13,  110,   12,  201,\n",
       "         22,   25,    2,   14,   74,    7,   23,   26,   16,   75,   11,\n",
       "          0,   47,    4,   43,   27,   37,   39,  101,    8,   99,  112,\n",
       "          6,  120,   62,   15,   50,   20,   17,   33,   40,   79,   94,\n",
       "          3,  500,   52,   78,   77,   96,   70,  291,   38,  170,  203,\n",
       "        237,  104,  103,  220,   60,  366,   49,    9,   18,  145,    5,\n",
       "        147,  175,  153,  102,   44,   36,  193,  167,   30,   42,   21,\n",
       "         48,  224,   41,   35,  178,   45,  113,  127,  293,   46,   65,\n",
       "         34,  258,  195,  161,  124,   61,  131,   97,  114,   69,  109,\n",
       "         73,   76,  150, 1787,  154,   31,  373,  243,   29,  100,  276,\n",
       "         58,  128,   54,  180,   92,   19,  182,   72,  160,  115,   53,\n",
       "        296,   91,   55,   59,  331,  694,  358,  142,  155,  305,  496,\n",
       "         32,  140,   86,  105,   85,   67,   63,  137,  192,  146,  136,\n",
       "        214,   68,   56,  151,  726,  108,   95,  163,  227,  191,   87,\n",
       "        330,  172,  365,  380,  400,  526,   83,  475,   66,   93,  300,\n",
       "         84,   80, 1471,  156,   89,  208,   98,  164])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['num_episodes'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Handling of Unicode Characters in Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nunicode_list = [(char, f\"U+{ord(char):04X}\") for char in test]\\n\\n# Print the results\\nfor char, code in unicode_list:\\n    print(f\"\\'{char}\\' -> {code}\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" List the Unicode code points of the characters in a string\n",
    "unicode_list = [(char, f\"U+{ord(char):04X}\") for char in test]\n",
    "\n",
    "# Print the results\n",
    "for char, code in unicode_list:\n",
    "    print(f\"'{char}' -> {code}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(test)\n",
    "encoding['input_ids']\n",
    "output = tokenizer.convert_ids_to_tokens(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Processed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data.to_csv(os.path.join(data_dir, \"interim\", \"anime_data_processed.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "data_path = os.path.join(data_dir, \"raw\", \"anime_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'id,' # Anime ID (integer)\\\n",
    "'title,' # Anime title (string)\\\n",
    "✅'synopsis,' # Anime synopsis (string or null)\\\n",
    "'mean,' # Mean score (float or null)\\\n",
    "'popularity,' # Popularity rank (integer or null)\\\n",
    "'num_list_users,' # Number of users who have the anime in their list (integer)\\\n",
    "'num_scoring_users,' # Number of users who have scored the anime (integer)\\\n",
    "✅'nsfw,' # NSFW classification (white=sfw, gray=partially, black=nsfw) (string or null)\\\n",
    "✅'genres,' # Genres (array of objects)\\\n",
    "✅'studios,' # Studios (array of objects)\\\n",
    "'num_episodes,' # Number of episodes (integer)\\\n",
    "'average_episode_duration,' # Average duration of an episode (integer or null)\\\n",
    "✅'status,' # Airing status (string)\\\n",
    "✅'rating,' # Age rating (string or null) (g, pg, pg_13, r, r+, rx)\\\n",
    "✅'source,' # Source (string or null)\\\n",
    "✅'media_type,' # Media type (string)\\\n",
    "'created_at,' # Date of creation (string <date-time>)\\\n",
    "'updated_at,' # Date of last update (string <date-time>)\\\n",
    "'start_season,' # Start season (object or null)\\\n",
    "'start_date,' # Start date (string or null)\\\n",
    "'end_date,' # End date (string or null)\\\n",
    "'related_anime,' # Related anime (array of objects)\\\n",
    "'related_manga,' # Related manga (array of objects)\\\n",
    "'recommendations,' # Recommendations (array of objects)\\\n",
    "'statistics' # Statistics (object or null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(data_path)\n",
    "train_data = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Date Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fill/fix nan/nonsensical values\n",
    "  - maybe encode start month/day and end month/day as cyclic features\n",
    "  - for anime with only start year: copy year over to end_date\n",
    "  - for anime anime with negative duration: swap start_date and end_date\n",
    "  - check start_season.year for starting year information; copy this over and drop start_season.year\n",
    "- encode date information\n",
    "  - encode date either as cyclical feature or something like decimal year\n",
    "  - decimal year: year + dayOfTheYear/365 (for leap-day use 0.5 of a day, e.g. 59.5/365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12266\n",
      "10085\n",
      "434\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_data))\n",
    "print(len(raw_data.dropna()))\n",
    "print(len(raw_data[raw_data['synopsis'].isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_encode(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Start Season\n",
    "['start_season,' (object or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['start_season.season'] = train_data['start_season.season'].fillna('Unknown')\n",
    "train_data['start_season.year'] = train_data['start_season.year'].fillna(train_data['start_season.year'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_encoder = LabelEncoder()\n",
    "season_encoder.fit(train_data['start_season.season'].unique())\n",
    "train_data['start_season.season'] = season_encoder.transform(train_data['start_season.season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Synopsis\n",
    "['synopsis,' (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = unicodedata.normalize('NFKC', text)  # Unicode normalization\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags if any\n",
    "    text = re.sub(r\"\\(.*source.*\\)\", \"\", text, flags=re.IGNORECASE)  # Remove source citations\n",
    "    text = re.sub(r\"\\[.*MAL.*\\]\", \"\", text)  # Remove MAL citations\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "    text = text.strip()  # Strip whitespace from the beginning and the end\n",
    "    return text\n",
    "\n",
    "train_data['synopsis'] = train_data['synopsis'].fillna(\"\")\n",
    "train_data['synopsis'] = train_data['synopsis'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Genre Lables\n",
    "['genres,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['genres'] = train_data['genres'].fillna('[]')\n",
    "train_data['genres'] = train_data['genres'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = set()\n",
    "\n",
    "def process(entry):\n",
    "    genres_set = set(genre['name'] for genre in entry)\n",
    "    unique_genres.update(genres_set)\n",
    "    return genres_set\n",
    "\n",
    "train_data['genres'] = train_data['genres'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mlb = MultiLabelBinarizer()\n",
    "genre_mlb.fit([unique_genres])\n",
    "\n",
    "train_data['genres'] = train_data['genres'].apply(lambda x: np.squeeze(genre_mlb.transform([x])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Studio Labels\n",
    "['studios,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['studios'] = train_data['studios'].fillna('[]')\n",
    "train_data['studios'] = train_data['studios'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_studios = set()\n",
    "\n",
    "def process(entry):\n",
    "    studios_set = set(studio['name'] for studio in entry)\n",
    "    unique_studios.update(studios_set)\n",
    "    return studios_set\n",
    "\n",
    "train_data['studios'] = train_data['studios'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_mlb = MultiLabelBinarizer()\n",
    "studio_mlb.fit([unique_studios])\n",
    "\n",
    "train_data['studios'] = train_data['studios'].apply(lambda x: np.squeeze(studio_mlb.transform([x])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess NSFW Tag\n",
    "['nsfw,' (white=sfw, gray=partially, black=nsfw) (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['nsfw'] = train_data['nsfw'].fillna(\"Unknown\")\n",
    "nsfw_encoder = LabelEncoder()\n",
    "nsfw_encoder.fit(train_data['nsfw'].unique())\n",
    "train_data['nsfw'] = nsfw_encoder.transform(train_data['nsfw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Source\n",
    "['source,' (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['source'] = train_data['source'].fillna(\"Unknown\")\n",
    "source_encoder = LabelEncoder()\n",
    "source_encoder.fit(train_data['source'].unique())\n",
    "train_data['source'] = source_encoder.transform(train_data['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Status\n",
    "['status,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_encoder = LabelEncoder()\n",
    "status_encoder.fit(train_data['status'].unique())\n",
    "train_data['status'] = status_encoder.transform(train_data['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Media Type\n",
    "['media_type,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_type_encoder = LabelEncoder()\n",
    "media_type_encoder.fit(train_data['media_type'].unique())\n",
    "train_data['media_type'] = media_type_encoder.transform(train_data['media_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Rating\n",
    "['rating,' (string or null) (g, pg, pg_13, r, r+, rx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_map = {\n",
    "    \"g\": 0,\n",
    "    \"pg\": 1,\n",
    "    \"pg_13\": 2,\n",
    "    \"r\": 3,\n",
    "    \"r+\": 4,\n",
    "    \"rx\": 5\n",
    "}\n",
    "train_data['rating'] = train_data['rating'].fillna(\"Unknown\")\n",
    "train_data['rating'] = train_data['rating'].map(rating_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Numerical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Num Episodes ['num_episodes,' (integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['num_episodes'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Handling of Unicode Characters in Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" List the Unicode code points of the characters in a string\n",
    "unicode_list = [(char, f\"U+{ord(char):04X}\") for char in test]\n",
    "\n",
    "# Print the results\n",
    "for char, code in unicode_list:\n",
    "    print(f\"'{char}' -> {code}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(test)\n",
    "encoding['input_ids']\n",
    "output = tokenizer.convert_ids_to_tokens(encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Processed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data.to_csv(os.path.join(data_dir, \"interim\", \"anime_data_processed.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

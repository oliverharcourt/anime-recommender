{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import textacy.preprocessing as tprep\n",
    "from transformers import DistilBertTokenizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "data_path = os.path.join(data_dir, \"raw\", \"anime_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'id,' # Anime ID (integer)\\\n",
    "ðŸ›‘'title,' # Anime title (string)\\\n",
    "âœ…'synopsis,' # Anime synopsis (string or null)\\\n",
    "âœ…'mean,' # Mean score (float or null)\\\n",
    "âœ…'popularity,' # Popularity rank (integer or null)\\\n",
    "ðŸ›‘'num_list_users,' # Number of users who have the anime in their list (integer)\\\n",
    "âœ…'num_scoring_users,' # Number of users who have scored the anime (integer)\\\n",
    "âœ…'nsfw,' # NSFW classification (white=sfw, gray=partially, black=nsfw) (string or null)\\\n",
    "âœ…'genres,' # Genres (array of objects)\\\n",
    "âœ…'studios,' # Studios (array of objects)\\\n",
    "âœ…'num_episodes,' # Number of episodes (integer)\\\n",
    "âœ…'average_episode_duration,' # Average duration of an episode (integer or null)\\\n",
    "âœ…'status,' # Airing status (string)\\\n",
    "âœ…'rating,' # Age rating (string or null) (g, pg, pg_13, r, r+, rx)\\\n",
    "âœ…'source,' # Source (string or null)\\\n",
    "âœ…'media_type,' # Media type (string)\\\n",
    "ðŸ›‘'created_at,' # Date of creation (string <date-time>)\\\n",
    "ðŸ›‘'updated_at,' # Date of last update (string <date-time>)\\\n",
    "âœ…'start_season,' # Start season (object or null)\\\n",
    "âœ…'start_date,' # Start date (string or null)\\\n",
    "âœ…'end_date,' # End date (string or null)\\\n",
    "ðŸ›‘'related_anime,' # Related anime (array of objects)\\\n",
    "ðŸ›‘'related_manga,' # Related manga (array of objects)\\\n",
    "ðŸ›‘'recommendations,' # Recommendations (array of objects)\\\n",
    "âœ…'statistics' # Statistics (object or null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(data_path)\n",
    "# For autoencoder training we drop all NaNs\n",
    "raw_data = raw_data.dropna()\n",
    "raw_data = raw_data.drop(columns=['title', 'created_at', 'updated_at', 'related_anime', 'related_manga', 'recommendations', 'main_picture.medium', 'main_picture.large'])\n",
    "train_data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10085\n",
      "10085\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_data))\n",
    "print(len(raw_data.dropna()))\n",
    "print(len(raw_data[raw_data['synopsis'].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Date/Season Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dates\n",
    "['start_season,' # Start season (object or null)]\\\n",
    "['start_date,' # Start date (string or null)]\\\n",
    "['end_date,' # End date (string or null)]\\\n",
    "['start_season.year,' # Start year (object or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_date_convert(date) -> datetime.date:\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    if type(date) is float:\n",
    "        return datetime.strptime(str(int(date)), '%Y').date()\n",
    "    if type(date) is str:\n",
    "        if re.compile(\"\\d{4}-\\d{2}-\\d{2}\").match(date):\n",
    "            return datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        elif re.compile(\"\\d{4}-\\d{2}\").search(date):\n",
    "            return datetime.strptime(date, '%Y-%m').date()\n",
    "        else:\n",
    "            return datetime.strptime(date, '%Y').date()\n",
    "    raise ValueError(f\"Invalid date format: {date}, {type(date)}\")\n",
    "\n",
    "train_data['start_season.year'] = train_data['start_season.year'].apply(safe_date_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(start_date, end_date):\n",
    "    if pd.isna(start_date) or pd.isna(end_date):\n",
    "        return None\n",
    "    if start_date <= end_date:\n",
    "        return (end_date - start_date).days\n",
    "    else:\n",
    "        return (start_date - end_date).days\n",
    "\n",
    "train_data['start_date'] = train_data['start_date'].apply(safe_date_convert)\n",
    "train_data['end_date'] = train_data['end_date'].apply(safe_date_convert)\n",
    "# Calculate time difference\n",
    "train_data['time_diff'] = train_data.apply(lambda x: time_diff(x['start_date'], x['end_date']), axis=1)\n",
    "train_data = train_data.drop(columns=['start_date', 'end_date'])\n",
    "\n",
    "# TODO scale time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO handle start_season.year ==> max-min-scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Start Season\n",
    "['start_season,' (object or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_encode(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data\n",
    "\n",
    "season_encoder = LabelEncoder()\n",
    "train_data['start_season.season'] = season_encoder.fit_transform(train_data['start_season.season'])\n",
    "\n",
    "# Apply the cyclical_encode function to create sine and cosine features\n",
    "train_data = cyclical_encode(train_data, 'start_season.season', max_val=len(season_encoder.classes_))\n",
    "train_data = train_data.drop(columns=['start_season.season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Synopsis\n",
    "['synopsis,' (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = unicodedata.normalize('NFKC', text)  # Unicode normalization\n",
    "    text = tprep.normalize.hyphenated_words(text)  # Normalize hyphenated words\n",
    "    text = tprep.normalize.quotation_marks(text)  # Normalize quotation marks\n",
    "    text = tprep.remove.accents(text)  # Remove accents\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags if any\n",
    "    text = re.sub(r\"\\(.*source.*\\)\", \"\", text, flags=re.IGNORECASE)  # Remove source citations\n",
    "    text = re.sub(r\"\\[.*MAL.*\\]\", \"\", text)  # Remove MAL citations\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "    text = text.strip()  # Strip whitespace from the beginning and the end\n",
    "    return text\n",
    "\n",
    "train_data['synopsis'] = train_data['synopsis'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "train_data['tokenized_synopsis'] = train_data['synopsis'].apply(lambda x: tokenizer(x, truncation=True, padding=True, max_length=512, return_tensors='pt') if not pd.isna(x) else None)\n",
    "train_data = train_data.drop(columns=['synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                             52991\n",
       "mean                                                                        3.121522\n",
       "popularity                                                                 -1.341264\n",
       "num_scoring_users                                                           1.685682\n",
       "nsfw                                                                               1\n",
       "genres                             [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
       "studios                            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "num_episodes                                                                1.297257\n",
       "average_episode_duration                                                      0.2379\n",
       "status                                                                             1\n",
       "rating                                                                             2\n",
       "source                                                                             5\n",
       "media_type                                                                         7\n",
       "start_season.year                                                         2023-01-01\n",
       "statistics.status.watching                                                  2.448078\n",
       "statistics.status.completed                                                 1.283366\n",
       "statistics.status.on_hold                                                   1.552597\n",
       "statistics.status.dropped                                                   1.314327\n",
       "statistics.status.plan_to_watch                                             1.894968\n",
       "statistics.num_list_users                                                   1.774003\n",
       "time_diff                                                                        175\n",
       "start_season.season_sin                                                          0.0\n",
       "start_season.season_cos                                                          1.0\n",
       "tokenized_synopsis                                       [input_ids, attention_mask]\n",
       "statistics.sum                                                              1.774003\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Tag/Label Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Genre Lables\n",
    "['genres,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genres = set()\n",
    "\n",
    "def process(entry):\n",
    "    genres_set = set(genre['name'] for genre in entry)\n",
    "    unique_genres.update(genres_set)\n",
    "    return genres_set\n",
    "\n",
    "train_data['genres'] = train_data['genres'].apply(ast.literal_eval)\n",
    "train_data['genres'] = train_data['genres'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_mlb = MultiLabelBinarizer()\n",
    "genre_mlb.fit([unique_genres])\n",
    "\n",
    "train_data['genres'] = train_data['genres'].apply(lambda x: np.squeeze(genre_mlb.transform([x])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Studio Labels\n",
    "['studios,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_studios = set()\n",
    "\n",
    "def process(entry):\n",
    "    studios_set = set(studio['name'] for studio in entry)\n",
    "    unique_studios.update(studios_set)\n",
    "    return studios_set\n",
    "\n",
    "train_data['studios'] = train_data['studios'].apply(ast.literal_eval)\n",
    "train_data['studios'] = train_data['studios'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "studio_mlb = MultiLabelBinarizer()\n",
    "studio_mlb.fit([unique_studios])\n",
    "\n",
    "train_data['studios'] = train_data['studios'].apply(lambda x: np.squeeze(studio_mlb.transform([x])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess NSFW Tag\n",
    "['nsfw,' (white=sfw, gray=partially, black=nsfw) (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsfw_encoder = LabelEncoder()\n",
    "nsfw_encoder.fit(train_data['nsfw'].unique())\n",
    "train_data['nsfw'] = nsfw_encoder.transform(train_data['nsfw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Source\n",
    "['source,' (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_encoder = LabelEncoder()\n",
    "source_encoder.fit(train_data['source'].unique())\n",
    "train_data['source'] = source_encoder.transform(train_data['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Status\n",
    "['status,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_encoder = LabelEncoder()\n",
    "status_encoder.fit(train_data['status'].unique())\n",
    "train_data['status'] = status_encoder.transform(train_data['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Media Type\n",
    "['media_type,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_type_encoder = LabelEncoder()\n",
    "media_type_encoder.fit(train_data['media_type'].unique())\n",
    "train_data['media_type'] = media_type_encoder.transform(train_data['media_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Rating\n",
    "['rating,' (string or null) (g, pg, pg_13, r, r+, rx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_map = {\n",
    "    \"g\": 0,\n",
    "    \"pg\": 1,\n",
    "    \"pg_13\": 2,\n",
    "    \"r\": 3,\n",
    "    \"r+\": 4,\n",
    "    \"rx\": 5\n",
    "}\n",
    "train_data['rating'] = train_data['rating'].map(rating_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Numerical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Mean\n",
    "['mean,' (float or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature looks similar to a normal distribution, so we try a standard scaler\n",
    "mean_scaler = StandardScaler()\n",
    "train_data['mean'] = mean_scaler.fit_transform(train_data['mean'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Popularity\n",
    "['popularity,' (integer or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of this feature seems to get messed up for anything other then standard scaler\n",
    "popularity_scaler = StandardScaler()\n",
    "train_data['popularity'] = popularity_scaler.fit_transform(train_data['popularity'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Number of Users Who Have Scored The Anime\n",
    "['num_scoring_users,' (integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature exhibits a long tail distribution, we try power transformer (yeo-johnson)\n",
    "# This might not the best way to handle this feature\n",
    "# Perhaps try https://arxiv.org/abs/2111.05956#:~:text=The%20visual%20world%20naturally%20exhibits,models%20based%20on%20deep%20learning.\n",
    "\n",
    "popularity_scaler = PowerTransformer()\n",
    "train_data['num_scoring_users'] = popularity_scaler.fit_transform(train_data['num_scoring_users'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Number of Episodes\n",
    "['num_episodes,' (integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature exhibits a long tail distribution, we again try power transformer (yeo-johnson)\n",
    "num_episodes_scaler = PowerTransformer()\n",
    "train_data['num_episodes'] = num_episodes_scaler.fit_transform(train_data['num_episodes'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Average Episode Duration\n",
    "['average_episode_duration,' (integer or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature might also benefit from power transformer (yeo-johnson)\n",
    "avg_ep_scaler = PowerTransformer()\n",
    "train_data['average_episode_duration'] = avg_ep_scaler.fit_transform(train_data['average_episode_duration'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Statistics\n",
    "'statistics' (object or null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature 'num_list_users' contains inconsistent data\n",
    "# We will drop this feature, and instead create a new feature 'statistics.sum'\n",
    "train_data = train_data.drop(columns=['num_list_users'])\n",
    "#train_data['statistics.sum'] = train_data['statistics.status.watching'] + train_data['statistics.status.completed'] + train_data['statistics.status.on_hold'] + train_data['statistics.status.dropped'] + train_data['statistics.status.plan_to_watch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "watching_scaler = PowerTransformer()\n",
    "train_data['statistics.status.watching'] = watching_scaler.fit_transform(train_data['statistics.status.watching'].values.reshape(-1, 1))\n",
    "completed_scaler = PowerTransformer()\n",
    "train_data['statistics.status.completed'] = completed_scaler.fit_transform(train_data['statistics.status.completed'].values.reshape(-1, 1))\n",
    "on_hold_scaler = PowerTransformer()\n",
    "train_data['statistics.status.on_hold'] = on_hold_scaler.fit_transform(train_data['statistics.status.on_hold'].values.reshape(-1, 1))\n",
    "dropped_scaler = PowerTransformer()\n",
    "train_data['statistics.status.dropped'] = dropped_scaler.fit_transform(train_data['statistics.status.dropped'].values.reshape(-1, 1))\n",
    "plan_to_watch_scaler = PowerTransformer()\n",
    "train_data['statistics.status.plan_to_watch'] = plan_to_watch_scaler.fit_transform(train_data['statistics.status.plan_to_watch'].values.reshape(-1, 1))\n",
    "num_list_users_scaler = PowerTransformer()\n",
    "train_data['statistics.num_list_users'] = num_list_users_scaler.fit_transform(train_data['statistics.num_list_users'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Processed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(os.path.join(data_dir, \"interim\", \"anime_data_processed.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

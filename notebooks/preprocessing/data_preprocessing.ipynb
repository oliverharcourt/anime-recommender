{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import textacy.preprocessing as tprep\n",
    "from transformers import DistilBertTokenizer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "root_dir = os.getenv(\"ROOT_DIR\")\n",
    "data_path = os.path.join(data_dir, \"interim\", \"interim_full_anime_data.json\")\n",
    "raw_data = pd.read_json(data_path, orient='records')\n",
    "# For autoencoder training we drop all NaNs\n",
    "raw_data = raw_data.dropna()\n",
    "# Drop irrelevant columns\n",
    "#raw_data = raw_data.drop(columns=['created_at', 'updated_at', 'related_manga', 'recommendations', 'main_picture.medium', 'main_picture.large'])\n",
    "train_data, test_data = train_test_split(raw_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime features and their format\n",
    "\n",
    "'id,' # Anime ID (integer)\\\n",
    "âœ…'title,' # Anime title (string)\\\n",
    "âœ…'synopsis,' # Anime synopsis (string or null)\\\n",
    "âœ…'mean,' # Mean score (float or null)\\\n",
    "âœ…'popularity,' # Popularity rank (integer or null)\\\n",
    "âœ…'num_list_users,' # Number of users who have the anime in their list (integer)\\\n",
    "âœ…'num_scoring_users,' # Number of users who have scored the anime (integer)\\\n",
    "âœ…'nsfw,' # NSFW classification (white=sfw, gray=partially, black=nsfw) (string or null)\\\n",
    "âœ…'genres,' # Genres (array of objects)\\\n",
    "âœ…'studios,' # Studios (array of objects)\\\n",
    "âœ…'num_episodes,' # Number of episodes (integer)\\\n",
    "âœ…'average_episode_duration,' # Average duration of an episode (integer or null)\\\n",
    "âœ…'status,' # Airing status (string)\\\n",
    "âœ…'rating,' # Age rating (string or null) (g, pg, pg_13, r, r+, rx)\\\n",
    "âœ…'source,' # Source (string or null)\\\n",
    "âœ…'media_type,' # Media type (string)\\\n",
    "ðŸ›‘'created_at,' # Date of creation (string <date-time>)\\\n",
    "ðŸ›‘'updated_at,' # Date of last update (string <date-time>)\\\n",
    "âœ…'start_season,' # Start season (object or null)\\\n",
    "âœ…'start_date,' # Start date (string or null)\\\n",
    "âœ…'end_date,' # End date (string or null)\\\n",
    "ðŸš«(no longer supported)'related_anime,' # Related anime (array of objects)\\\n",
    "ðŸš«(no longer supported)'related_manga,' # Related manga (array of objects)\\\n",
    "ðŸš«(no longer supported)'recommendations,' # Recommendations (array of objects)\\\n",
    "ðŸš«(no longer supported)'statistics' # Statistics (object or null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['simple'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name, sub_dict=None, save=True):\n",
    "    if not sub_dict:\n",
    "        sub_dict = ''\n",
    "    model_name_clean = model_name.replace('.', '_')\n",
    "    model_path = os.path.join(root_dir, 'src', 'preprocessing', 'models', sub_dict, model_name_clean + '.pkl')\n",
    "    if sub_dict:\n",
    "        config[sub_dict][model_name] = model_path\n",
    "    else:\n",
    "        config[model_name] = model_path\n",
    "    if save:\n",
    "        with open(model_path, 'wb') as model_file:\n",
    "            pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Date/Season Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dates\n",
    "['start_season,' # Start season (object or null)]\\\n",
    "['start_date,' # Start date (string or null)]\\\n",
    "['end_date,' # End date (string or null)]\\\n",
    "['start_season.year,' # Start year (object or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dates(data, save=True):\n",
    "    def safe_date_convert(date) -> datetime.date:\n",
    "        if pd.isna(date):\n",
    "            return None\n",
    "        if type(date) is float:\n",
    "            return datetime.strptime(str(int(date)), '%Y').date()\n",
    "        if type(date) is str:\n",
    "            if re.compile(\"\\d{4}-\\d{2}-\\d{2}\").match(date):\n",
    "                return datetime.strptime(date, '%Y-%m-%d').date()\n",
    "            elif re.compile(\"\\d{4}-\\d{2}\").search(date):\n",
    "                return datetime.strptime(date, '%Y-%m').date()\n",
    "            else:\n",
    "                return datetime.strptime(date, '%Y').date()\n",
    "        raise ValueError(f\"Invalid date format: {date}, {type(date)}\")\n",
    "\n",
    "    def time_diff(start_date, end_date):\n",
    "        if pd.isna(start_date) or pd.isna(end_date):\n",
    "            return None\n",
    "        if start_date <= end_date:\n",
    "            return (end_date - start_date).days\n",
    "        else:\n",
    "            return (start_date - end_date).days\n",
    "\n",
    "    # Convert dates to datetime objects\n",
    "    data['start_date'] = data['start_date'].apply(safe_date_convert)\n",
    "    data['end_date'] = data['end_date'].apply(safe_date_convert)\n",
    "    # Calculate time difference\n",
    "    data['time_diff'] = data.apply(lambda x: time_diff(x['start_date'], x['end_date']), axis=1)\n",
    "    data = data.drop(columns=['start_date', 'end_date'])\n",
    "    # Scale time_diff\n",
    "    td_scaler = PowerTransformer()\n",
    "    data['time_diff'] = td_scaler.fit_transform(data['time_diff'].values.reshape(-1, 1))\n",
    "    save_model(td_scaler, 'time_diff', save=save)\n",
    "    # Scale start_season.year\n",
    "    year_scaler = MinMaxScaler()\n",
    "    data['start_season.year'] = year_scaler.fit_transform(data['start_season.year'].values.reshape(-1, 1))\n",
    "    save_model(year_scaler, 'start_season_year', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Start Season\n",
    "['start_season,' (object or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_season(data, save=True):\n",
    "    def cyclical_encode(data, col, max_val):\n",
    "        data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "        data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "        return data\n",
    "\n",
    "    season_encoder = LabelEncoder()\n",
    "    data['start_season.season'] = season_encoder.fit_transform(data['start_season.season'])\n",
    "    save_model(season_encoder, 'start_season_season', save=save)\n",
    "\n",
    "    # Apply the cyclical_encode function to create sine and cosine features\n",
    "    data = cyclical_encode(data, 'start_season.season', max_val=len(season_encoder.classes_))\n",
    "    data = data.drop(columns=['start_season.season'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text\n",
    "['synopsis,' (string or null)]\\\n",
    "related column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    def clean_text(text):\n",
    "        text = unicodedata.normalize('NFKC', text)  # Unicode normalization\n",
    "        text = text.replace('\\u2013', '\\u002d')  # Replace en dash with hyphen\n",
    "        text = text.replace('\\u00d7', '\\u0078')  # Replace multiplication sign with x\n",
    "        text = tprep.normalize.hyphenated_words(text)  # Normalize hyphenated words\n",
    "        text = tprep.normalize.quotation_marks(text)  # Normalize quotation marks\n",
    "        text = tprep.normalize.bullet_points(text)  # Normalize bullet points\n",
    "        text = tprep.normalize.whitespace(text)  # Normalize whitespace\n",
    "        text = tprep.remove.accents(text)  # Remove accents\n",
    "        text = re.sub(r'<.*?>', '', text)  # Remove HTML tags if any\n",
    "        text = re.sub(r\"\\([\\s+]?source.*?\\)+\", \"\", text, flags=re.IGNORECASE)  # Remove source citations\n",
    "        text = re.sub(r\"\\[Writ.*?by.*?\\]\", \"\", text)  # Remove MAL citations\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "        text = text.strip()  # Strip whitespace from the beginning and the end\n",
    "        return text\n",
    "    \n",
    "    def preprocess_related(data):\n",
    "        # f = lambda x: [entry['node']['title'] for entry in ast.literal_eval(x)]\n",
    "        # cr = lambda x: [clean_text(i) for i in f(x)]\n",
    "        g = lambda x: [clean_text(x)]\n",
    "        # data['related'] = data['title'].apply(g) + data['related_anime'].apply(cr)\n",
    "        # data['related'] = data['related'].apply(sorted)\n",
    "        data['related'] = data['title'].apply(g)\n",
    "        data = data.drop(columns=['title'])#, 'related_anime'])\n",
    "        return data\n",
    "\n",
    "    data['synopsis'] = data['synopsis'].apply(clean_text)\n",
    "    data = preprocess_related(data)\n",
    "    # data['related'] = data['related'].apply(' '.join)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Tag & Label Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Genre Lables\n",
    "['genres,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_genres(data, save=True):\n",
    "    genres = {\n",
    "        'Action', 'Adventure', 'Avant Garde', 'Award Winning', 'Boys Love', 'Comedy',\n",
    "        'Drama', 'Fantasy', 'Girls Love', 'Gourmet', 'Horror', 'Mystery', 'Romance',\n",
    "        'Sci-Fi',\n",
    "        'Slice of Life',\n",
    "        'Sports',\n",
    "        'Supernatural',\n",
    "        'Suspense',\n",
    "        'Ecchi',\n",
    "        'Erotica',\n",
    "        'Hentai',\n",
    "        'Adult Cast',\n",
    "        'Anthropomorphic',\n",
    "        'CGDCT',\n",
    "        'Childcare',\n",
    "        'Combat Sports',\n",
    "        'Crossdressing',\n",
    "        'Delinquents',\n",
    "        'Detective',\n",
    "        'Educational',\n",
    "        'Gag Humor',\n",
    "        'Gore',\n",
    "        'Harem',\n",
    "        'High Stakes Game',\n",
    "        'Historical',\n",
    "        'Idols (Female)',\n",
    "        'Idols (Male)',\n",
    "        'Isekai',\n",
    "        'Iyashikei',\n",
    "        'Love Polygon',\n",
    "        'Magical Sex Shift',\n",
    "        'Mahou Shoujo',\n",
    "        'Martial Arts',\n",
    "        'Mecha',\n",
    "        'Medical',\n",
    "        'Military',\n",
    "        'Music',\n",
    "        'Mythology',\n",
    "        'Organized Crime',\n",
    "        'Otaku Culture',\n",
    "        'Parody',\n",
    "        'Performing Arts',\n",
    "        'Pets',\n",
    "        'Psychological',\n",
    "        'Racing',\n",
    "        'Reincarnation',\n",
    "        'Reverse Harem',\n",
    "        'Romantic Subtext',\n",
    "        'Samurai',\n",
    "        'School',\n",
    "        'Showbiz',\n",
    "        'Space',\n",
    "        'Strategy Game',\n",
    "        'Super Power',\n",
    "        'Survival',\n",
    "        'Team Sports',\n",
    "        'Time Travel',\n",
    "        'Vampire',\n",
    "        'Video Game',\n",
    "        'Visual Arts',\n",
    "        'Workplace',\n",
    "        'Josei',\n",
    "        'Kids',\n",
    "        'Seinen',\n",
    "        'Shoujo',\n",
    "        'Shounen',\n",
    "    }\n",
    "\n",
    "    def process(entry):\n",
    "        genres_set = set(genre['name'] for genre in entry)\n",
    "        return genres_set\n",
    "\n",
    "    # data['genres'] = data['genres'].apply(ast.literal_eval)\n",
    "    data['genres'] = data['genres'].apply(process)\n",
    "    \n",
    "    genre_mlb = MultiLabelBinarizer()\n",
    "    genre_mlb.fit([genres])\n",
    "    data['genres'] = data['genres'].apply(lambda x: genre_mlb.transform([x]).reshape(1, -1))\n",
    "    save_model(genre_mlb, 'genres', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Studio Labels\n",
    "['studios,' (array of objects, may be empty)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_studios(data, save=True):\n",
    "\n",
    "    def process(entry):\n",
    "        studios_set = [studio['name'] for studio in entry]\n",
    "        return studios_set\n",
    "\n",
    "    # data['studios'] = data['studios'].apply(ast.literal_eval)\n",
    "    data['studios'] = data['studios'].apply(process)\n",
    "\n",
    "    # Use FeatureHasher to encode studios\n",
    "    studio_hasher = FeatureHasher(n_features=75, input_type='string')\n",
    "    studios_hashed = studio_hasher.transform(data['studios']).toarray()\n",
    "    data['studios'] = [hash for hash in studios_hashed]\n",
    "    data['studios'] = data['studios'].apply(lambda x: x.reshape(1, -1))\n",
    "    save_model(studio_hasher, 'studios', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess NSFW Tag\n",
    "['nsfw,' (white=sfw, gray=partially, black=nsfw) (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nsfw(data, save=True):\n",
    "    nsfw_encoder = LabelEncoder()\n",
    "    data['nsfw'] = nsfw_encoder.fit_transform(data['nsfw'])\n",
    "    save_model(nsfw_encoder, 'nsfw', sub_dict='simple', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Source\n",
    "['source,' (string or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_source(data, save=True):\n",
    "    source_encoder = LabelEncoder()\n",
    "    sources = {\n",
    "        'other',\n",
    "        'original',\n",
    "        'manga',\n",
    "        '4_koma_manga',\n",
    "        'web_manga',\n",
    "        'digital_manga',\n",
    "        'novel',\n",
    "        'light_novel',\n",
    "        'visual_novel',\n",
    "        'game',\n",
    "        'card_game',\n",
    "        'book',\n",
    "        'picture_book',\n",
    "        'radio',\n",
    "        'music'\n",
    "    }\n",
    "    sources.update(data['source'].unique())\n",
    "    sources = list(sources)\n",
    "    source_encoder.fit(sources)\n",
    "    data['source'] = source_encoder.transform(data['source'])\n",
    "    save_model(source_encoder, 'source', sub_dict='simple', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Status\n",
    "['status,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_status(data, save=True):\n",
    "    status_encoder = LabelEncoder()\n",
    "    data['status'] = status_encoder.fit_transform(data['status'])\n",
    "    save_model(status_encoder, 'status', sub_dict='simple', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Media Type\n",
    "['media_type,' (string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_media_type(data, save=True):\n",
    "    # We might want to change media_type to better reflect the data\n",
    "    # We might use the following rules:\n",
    "    # movie = 'avg_ep_dur'>1800\n",
    "    # tv = ('avg_ep_dur'<=1800 & 'num_episodes'>=6)\n",
    "    # special = ('avg_ep_dur'<=1800 & 'num_episodes'<6) | ('avg_ep_dur' < 240)\n",
    "    # This covers all cases, but the duration and num_ep thresholds seem suboptimal after some testing\n",
    "    # thus we skip this for now\n",
    "\n",
    "    data['media_type'] = data['media_type'].apply(lambda x: 'special' if x in {'ona', 'ova', 'tv_special'} else x)\n",
    "    media_type_encoder = LabelEncoder()\n",
    "    data['media_type'] = media_type_encoder.fit_transform(data['media_type'])\n",
    "    save_model(media_type_encoder, 'media_type', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Rating\n",
    "['rating,' (string or null) (g, pg, pg_13, r, r+, rx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_rating(data):\n",
    "    rating_map = {\n",
    "        \"g\": 0,\n",
    "        \"pg\": 1,\n",
    "        \"pg_13\": 2,\n",
    "        \"r\": 3,\n",
    "        \"r+\": 4,\n",
    "        \"rx\": 5\n",
    "    }\n",
    "    data['rating'] = data['rating'].map(rating_map)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Numerical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Mean\n",
    "['mean,' (float or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mean(data, save=True):\n",
    "    # This feature looks similar to a normal distribution, so we try a standard scaler\n",
    "    mean_scaler = StandardScaler()\n",
    "    data['mean'] = mean_scaler.fit_transform(data['mean'].values.reshape(-1, 1))\n",
    "    save_model(mean_scaler, 'mean', sub_dict='simple', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Popularity\n",
    "['popularity,' (integer or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_popularity(data, save=True):\n",
    "    # The distribution of this feature seems to get messed up for anything other then standard scaler\n",
    "    popularity_scaler = StandardScaler()\n",
    "    data['popularity'] = popularity_scaler.fit_transform(data['popularity'].values.reshape(-1, 1))\n",
    "    save_model(popularity_scaler, 'popularity', sub_dict='simple', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Number of Users Who Have Scored The Anime\n",
    "['num_scoring_users,' (integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_num_scoring_users(data, save=True):\n",
    "    # This feature exhibits a long tail distribution, we try power transformer (yeo-johnson)\n",
    "    # This might not the best way to handle this feature\n",
    "    # Perhaps try https://arxiv.org/abs/2111.05956#:~:text=The%20visual%20world%20naturally%20exhibits,models%20based%20on%20deep%20learning.\n",
    "\n",
    "    popularity_scaler = PowerTransformer()\n",
    "    data['num_scoring_users'] = popularity_scaler.fit_transform(data['num_scoring_users'].values.reshape(-1, 1))\n",
    "    save_model(popularity_scaler, 'num_scoring_users', sub_dict='simple', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Number of Episodes\n",
    "['num_episodes,' (integer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_num_episodes(data, save=True):\n",
    "    # This feature exhibits a long tail distribution, we again try power transformer (yeo-johnson)\n",
    "    num_episodes_scaler = PowerTransformer()\n",
    "    data['num_episodes'] = num_episodes_scaler.fit_transform(data['num_episodes'].values.reshape(-1, 1))\n",
    "    save_model(num_episodes_scaler, 'num_episodes', sub_dict='simple', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Average Episode Duration\n",
    "['average_episode_duration,' (integer or null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_average_episode_duration(data, save=True):\n",
    "    # This feature might also benefit from power transformer (yeo-johnson)\n",
    "    avg_ep_scaler = PowerTransformer()\n",
    "    data['average_episode_duration'] = avg_ep_scaler.fit_transform(data['average_episode_duration'].values.reshape(-1, 1))\n",
    "    save_model(avg_ep_scaler, 'average_episode_duration', sub_dict='simple', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Statistics\n",
    "'statistics' (object or null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stats(data, save=True):\n",
    "    # The feature 'num_list_users' contains inconsistent data\n",
    "    # We will drop this feature, and instead create a new feature 'statistics.sum'\n",
    "    #data = data.drop(columns=['num_list_users'])\n",
    "    num_list_users_scaler = PowerTransformer()\n",
    "    data['num_list_users'] = num_list_users_scaler.fit_transform(data['num_list_users'].values.reshape(-1, 1))\n",
    "    save_model(num_list_users_scaler, 'num_list_users', sub_dict='simple', save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data, save=True, full=False):\n",
    "    # Save text data\n",
    "    data = preprocess_text(data)\n",
    "\n",
    "    # Special handling\n",
    "    data = preprocess_genres(data, save=save)\n",
    "    data = preprocess_studios(data, save=save)\n",
    "\n",
    "    if not full:\n",
    "        # Special handling\n",
    "        data = process_dates(data, save=save)\n",
    "        data = preprocess_season(data, save=save)\n",
    "        \n",
    "        # Simple features\n",
    "        data = preprocess_nsfw(data, save=save)\n",
    "        data = preprocess_source(data, save=save)\n",
    "        data = preprocess_status(data, save=save)\n",
    "        \n",
    "        # Special handling\n",
    "        data = preprocess_media_type(data, save=save)\n",
    "        data = preprocess_rating(data)\n",
    "        \n",
    "        # Simple features\n",
    "        data = preprocess_mean(data, save=save)\n",
    "        data = preprocess_popularity(data, save=save)\n",
    "        data = preprocess_num_scoring_users(data, save=save)\n",
    "        data = preprocess_num_episodes(data, save=save)\n",
    "        data = preprocess_average_episode_duration(data, save=save)\n",
    "        data = preprocess_stats(data, save=save)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "full = True\n",
    "if full:\n",
    "    #Â To just process the whole dataset:\n",
    "    # Make sure to disable the save_data method \n",
    "    main(raw_data, save=False, full=True)\n",
    "    raw_data.to_json(os.path.join(data_dir, \"processed\", \"full_data_processed.json\"), orient='records')\n",
    "else:\n",
    "    # To generate the scalers for training:\n",
    "    train = True\n",
    "    train_data = main(train_data)\n",
    "    train_data.to_json(os.path.join(data_dir, \"processed\", \"train_data_processed.json\"), orient='records')\n",
    "    json.dump(config, open(os.path.join(root_dir, 'configs', 'config_preprocessing.json'), 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

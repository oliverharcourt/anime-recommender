{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import textacy.preprocessing as tprep\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer, LabelEncoder, PowerTransformer, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv(\"DATA_DIR\")\n",
    "raw_data_path = os.path.join(data_dir, \"raw\", \"anime_data.csv\")\n",
    "raw_data = pd.read_csv(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'id,' # Anime ID (integer)\\\n",
    "'title,' # Anime title (string)\\\n",
    "âœ…'synopsis,' # Anime synopsis (string or null)\\\n",
    "âœ…'mean,' # Mean score (float or null)\\\n",
    "âœ…'popularity,' # Popularity rank (integer or null)\\\n",
    "ðŸ›‘'num_list_users,' # Number of users who have the anime in their list (integer)\\\n",
    "âœ…'num_scoring_users,' # Number of users who have scored the anime (integer)\\\n",
    "âœ…'nsfw,' # NSFW classification (white=sfw, gray=partially, black=nsfw) (string or null)\\\n",
    "âœ…'genres,' # Genres (array of objects)\\\n",
    "âœ…'studios,' # Studios (array of objects)\\\n",
    "âœ…'num_episodes,' # Number of episodes (integer)\\\n",
    "âœ…'average_episode_duration,' # Average duration of an episode (integer or null)\\\n",
    "âœ…'status,' # Airing status (string)\\\n",
    "âœ…'rating,' # Age rating (string or null) (g, pg, pg_13, r, r+, rx)\\\n",
    "âœ…'source,' # Source (string or null)\\\n",
    "âœ…'media_type,' # Media type (string)\\\n",
    "ðŸ›‘'created_at,' # Date of creation (string <date-time>)\\\n",
    "ðŸ›‘'updated_at,' # Date of last update (string <date-time>)\\\n",
    "âœ…'start_season,' # Start season (object or null)\\\n",
    "âœ…'start_date,' # Start date (string or null)\\\n",
    "âœ…'end_date,' # End date (string or null)\\\n",
    "'related_anime,' # Related anime (array of objects)\\\n",
    "'related_manga,' # Related manga (array of objects)\\\n",
    "'recommendations,' # Recommendations (array of objects)\\\n",
    "'statistics' # Statistics (object or null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Synopses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = unicodedata.normalize('NFKC', text)  # Unicode normalization\n",
    "    text = tprep.normalize.hyphenated_words(text)  # Normalize hyphenated words\n",
    "    text = tprep.normalize.quotation_marks(text)  # Normalize quotation marks\n",
    "    text = tprep.remove.accents(text)  # Remove accents\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags if any\n",
    "    text = re.sub(r\"\\(.*source.*\\)\", \"\", text, flags=re.IGNORECASE)  # Remove source citations\n",
    "    text = re.sub(r\"\\[.*MAL.*\\]\", \"\", text)  # Remove MAL citations\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "    text = text.strip()  # Strip whitespace from the beginning and the end\n",
    "    return text\n",
    "raw_data.dropna(inplace=True)\n",
    "raw_data['synopsis'] = raw_data['synopsis'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['synopsis'].to_csv(os.path.join(data_dir, \"interim\", \"synopsis.csv\"), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "raw_data['tokenized_synopsis'] = raw_data['synopsis'].apply(lambda x: tokenizer(x, truncation=True, max_length=512, return_tensors='pt') if not pd.isna(x) else None)\n",
    "raw_data['tokenized_synopsis_length'] = raw_data['tokenized_synopsis'].apply(lambda x: x['input_ids'].shape[1] if not pd.isna(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see how many synopses will be truncated by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_length(synopsis):\n",
    "    if pd.isna(synopsis):\n",
    "        return 0  # Return 0 for NaN values\n",
    "    return len(synopsis)\n",
    "\n",
    "raw_data['synopsis_length'] = raw_data['synopsis'].apply(safe_length)\n",
    "\n",
    "synopsis_length = raw_data['synopsis_length']\n",
    "sns.histplot(synopsis_length)\n",
    "plt.axvline(synopsis_length.mean(), c='k', ls='-', lw=2.5)\n",
    "plt.axvline(synopsis_length.median(), c='orange', ls='--', lw=2.5)\n",
    "raw_data['synopsis_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_length = raw_data['tokenized_synopsis_length']\n",
    "sns.histplot(tokenized_length)\n",
    "plt.axvline(tokenized_length.mean(), c='k', ls='-', lw=2.5)\n",
    "plt.axvline(tokenized_length.median(), c='orange', ls='--', lw=2.5)\n",
    "print(raw_data['tokenized_synopsis_length'].describe())\n",
    "print('Number of synopses of max tokenized length:', raw_data[raw_data['tokenized_synopsis_length'] == 512]['id'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normal(column=None, data=None):\n",
    "    if column is not None:\n",
    "        data = raw_data[column].fillna(raw_data[column].mean())\n",
    "    #res = stats.shapiro(data)\n",
    "    res2 = stats.normaltest(data)\n",
    "    if res2.pvalue > 0.05:\n",
    "        print(\"Normal\")\n",
    "    else:\n",
    "        print(\"Not Normal\")\n",
    "\n",
    "def plot(data, ax):\n",
    "    sns.histplot(data, ax=ax, log_scale=False)\n",
    "    #plt.axvline(data.mean(), c='k', ls='-', lw=1.5)\n",
    "    #plt.axvline(data.median(), c='orange', ls='--', lw=1.5)\n",
    "    #plt.xlim(0, data.quantile(.95))\n",
    "    #iqr = stats.iqr(data)\n",
    "    #lower_bound = data.quantile(.25) - 1.5 * iqr\n",
    "    #upper_bound = data.quantile(.75) + 1.5 * iqr\n",
    "    #outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    #print(f'Number of data points: {data.count()} Number of outliers: {outliers.count()}, {100*outliers.count()/data.count():.2f}%')\n",
    "    stats_df = pd.DataFrame(data).describe()\n",
    "    # Transpose for easier key-value access\n",
    "    stats_transposed = stats_df.T\n",
    "    stats_text = \"\\n\".join([f\"{stat}: {value:.2f}\" for stat, value in stats_transposed.iloc[0].items()])\n",
    "    ax.legend([stats_text], title='Descriptive Stats', loc='upper right', fontsize='small')\n",
    "    \n",
    "\n",
    "def analyze(column, type, fill='mean', post_st=False, clip=False):\n",
    "    if fill == 'mean':\n",
    "        data = raw_data[column].fillna(raw_data[column].mean())\n",
    "    elif fill == 'mode':\n",
    "        data = raw_data[column].fillna(raw_data[column].mode())\n",
    "    elif fill == 'median':\n",
    "        data = raw_data[column].fillna(raw_data[column].median())\n",
    "    else:\n",
    "        data = raw_data.dropna()\n",
    "    f, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    if clip:\n",
    "        q = raw_data[column].quantile(.95)\n",
    "        data = data[data <= q]\n",
    "    data = pd.DataFrame(data=data)\n",
    "    plot(data, axes[0])\n",
    "\n",
    "    if type == 'standard':\n",
    "        data = StandardScaler().fit_transform(data)\n",
    "    elif type == 'robust':\n",
    "        data = RobustScaler().fit_transform(data)\n",
    "    elif type == 'power':\n",
    "        data = PowerTransformer().fit_transform(data)\n",
    "    elif type == 'quantile':\n",
    "        data = QuantileTransformer().fit_transform(data)\n",
    "    elif type == 'log':\n",
    "        data = data.apply(np.log1p)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid type\")\n",
    "    \n",
    "    if post_st:\n",
    "        data = StandardScaler().fit_transform(data)\n",
    "\n",
    "    plot(data, axes[1])\n",
    "\n",
    "    f.suptitle(f'{column} - {type}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('mean')\n",
    "analyze('mean', 'standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### num_list_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_list_users does not match statistics.num_list_users ==> use statistics.num_list_users and drop num_list_users\n",
    "check_normal('num_list_users')\n",
    "analyze('num_list_users', 'robust')\n",
    "analyze('num_list_users', 'power')\n",
    "analyze('num_list_users', 'log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('popularity')\n",
    "analyze('popularity', 'standard')\n",
    "analyze('popularity', 'power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### num_scoring_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('num_scoring_users')\n",
    "analyze('num_scoring_users', 'log')\n",
    "analyze('num_scoring_users', 'power')\n",
    "analyze('num_scoring_users', 'standard')\n",
    "analyze('num_scoring_users', 'robust')\n",
    "analyze('num_scoring_users', 'quantile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('num_episodes')\n",
    "analyze('num_episodes', 'log')\n",
    "analyze('num_episodes', 'power')\n",
    "analyze('num_episodes', 'standard')\n",
    "analyze('num_episodes', 'robust')\n",
    "analyze('num_episodes', 'quantile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average_episode_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('average_episode_duration')\n",
    "analyze('average_episode_duration', 'log', post_st=True)\n",
    "analyze('average_episode_duration', 'power')\n",
    "analyze('average_episode_duration', 'robust')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = raw_data[['num_list_users', 'num_scoring_users', 'statistics.status.watching', 'statistics.status.completed', 'statistics.status.on_hold', 'statistics.status.dropped',\n",
    "       'statistics.status.plan_to_watch', 'statistics.num_list_users']]\n",
    "raw_data['sum'] = statistics['statistics.status.watching'] + statistics['statistics.status.completed'] + statistics['statistics.status.on_hold'] + statistics['statistics.status.dropped'] + statistics['statistics.status.plan_to_watch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal(data=raw_data['sum'])\n",
    "analyze('sum', 'log')\n",
    "analyze('sum', 'power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('statistics.status.watching')\n",
    "analyze('statistics.status.watching', 'standard')\n",
    "analyze('statistics.status.watching', 'power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('statistics.status.completed')\n",
    "analyze('statistics.status.completed', 'standard')\n",
    "analyze('statistics.status.completed', 'power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('statistics.status.on_hold')\n",
    "analyze('statistics.status.on_hold', 'standard')\n",
    "analyze('statistics.status.on_hold', 'power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('statistics.status.dropped')\n",
    "analyze('statistics.status.dropped', 'standard')\n",
    "analyze('statistics.status.dropped', 'quantile')\n",
    "analyze('statistics.status.dropped', 'power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal('statistics.status.plan_to_watch')\n",
    "analyze('statistics.status.plan_to_watch', 'standard')\n",
    "analyze('statistics.status.plan_to_watch', 'power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Date Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what status animes are in where the start date is missing\n",
    "raw_data[raw_data['start_date'].isna()]['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_date_convert(date) -> datetime.date:\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    if type(date) is float:\n",
    "        return datetime.strptime(str(int(date)), '%Y').date()\n",
    "    if type(date) is str:\n",
    "        if re.compile(\"\\d{4}-\\d{2}-\\d{2}\").match(date):\n",
    "            return datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        elif re.compile(\"\\d{4}-\\d{2}\").search(date):\n",
    "            return datetime.strptime(date, '%Y-%m').date()\n",
    "        else:\n",
    "            return datetime.strptime(date, '%Y').date()\n",
    "    raise ValueError(f\"Invalid date format: {date}, {type(date)}\")\n",
    "\n",
    "test = raw_data[['status', 'start_date', 'end_date', 'start_season.year', 'start_season.season']].dropna()\n",
    "test['start_season.year'].apply(safe_date_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Encoding Start Season as a Cyclical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclical_encode(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data\n",
    "\n",
    "# Encode the 'start_season.season' column using LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Create a new DataFrame to store the encoded labels and cyclical features\n",
    "encoded_data = raw_data.dropna().copy()\n",
    "encoded_labels = encoder.fit_transform(encoded_data['start_season.season'])\n",
    "encoded_data['encoded_season'] = encoded_labels\n",
    "\n",
    "# Apply the cyclical_encode function to create sine and cosine features\n",
    "encoded_data = cyclical_encode(encoded_data, 'encoded_season', max_val=len(encoder.classes_))\n",
    "\n",
    "# Print the encoded and transformed data\n",
    "print(encoded_data[['start_season.season', 'encoded_season', 'encoded_season_sin', 'encoded_season_cos']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.drop_duplicates(subset=['start_season.season'])[['start_season.season', 'encoded_season_sin', 'encoded_season_cos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values from the columns\n",
    "sin_values = encoded_data['encoded_season_sin']\n",
    "cos_values = encoded_data['encoded_season_cos']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the values on the unit circle\n",
    "ax.scatter(cos_values, sin_values)\n",
    "\n",
    "# Set the aspect ratio to equal\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Set the limits of the plot\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "\n",
    "# Add a unit circle\n",
    "circle = plt.Circle((0, 0), 1, color='black', fill=False)\n",
    "ax.add_artist(circle)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Cosine')\n",
    "ax.set_ylabel('Sine')\n",
    "ax.set_title('Values on the Unit Circle')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Start and End Date to Calculate Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(start_date, end_date):\n",
    "    if pd.isna(start_date) or pd.isna(end_date):\n",
    "        return None\n",
    "    if start_date <= end_date:\n",
    "        return (end_date - start_date).days\n",
    "    else:\n",
    "        return (start_date - end_date).days\n",
    "\n",
    "converted = test\n",
    "converted['start_date'] = converted['start_date'].apply(safe_date_convert)\n",
    "converted['end_date'] = converted['end_date'].apply(safe_date_convert)\n",
    "# calculate time difference\n",
    "raw_data['time_diff'] = converted.apply(lambda x: time_diff(x['start_date'], x['end_date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_normal(data=raw_data['time_diff'])\n",
    "analyze('time_diff', 'standard')\n",
    "analyze('time_diff', 'robust')\n",
    "analyze('time_diff', 'quantile')\n",
    "analyze('time_diff', 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duration of animes where full date information is available\n",
    "date_data = raw_data[raw_data['start_date'].notna() & raw_data['end_date'].notna()]\n",
    "date_data = date_data[date_data['start_date'].str.contains('-.*-') & date_data['end_date'].str.contains('-.*-')]\n",
    "date_data['start_date'] = pd.to_datetime(date_data['start_date'], yearfirst=True)\n",
    "date_data['end_date'] = pd.to_datetime(date_data['end_date'], yearfirst=True)\n",
    "date_data['duration'] = date_data['end_date'].dt.date - date_data['start_date'].dt.date\n",
    "date_data['duration'] = date_data['duration'].apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = datetime.strptime('2021-03', '%Y-%m').date()\n",
    "d2 = datetime.strptime('2021-04', '%Y-%m').date()\n",
    "print((d2 - d1).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for negative durations\n",
    "date_data.loc[(date_data['duration'] < 0), ['start_date', 'end_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Year Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=raw_data['start_season.year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO handle start_season.year ==> max-min-scaling?\n",
    "year_scaler = MinMaxScaler()\n",
    "data = year_scaler.fit_transform(raw_data['start_season.year'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
